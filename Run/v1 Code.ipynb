{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Belief Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf #Deep learning Library\n",
    "import numpy as np #Matrix Algebra Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras \n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(trX, trY) , (teX, teY) = mnist.load_data()\n",
    "print(trX.shape, trY.shape, teX.shape, teY.shape)\n",
    "trX = trX.reshape(trX.shape[0], trX.shape[1] * trX.shape[2])\n",
    "teX = teX.reshape(teX.shape[0], teX.shape[1] * teX.shape[2])\n",
    "print(trX.shape, trY.shape, teX.shape, teY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init__(self, input_size, output_size, learning_rate, batch_size):\n",
    "        self.input_size = input_size #Size of the input layer\n",
    "        self.output_size = output_size #Size of the hidden layer\n",
    "        self.epochs = 5 #How many times we will update the weights \n",
    "        self.learning_rate = learning_rate #How big of a weight update we will perform \n",
    "        self.batch_size = batch_size #How many images will we \"feature engineer\" at at time \n",
    "        self.new_input_layer = None #Initalize new input layer variable for k-step contrastive divergence \n",
    "        self.new_hidden_layer = None\n",
    "        self.new_test_hidden_layer = None\n",
    "        \n",
    "        self.w = np.random.normal(0,.01,[input_size,output_size]) #weights\n",
    "        self.hb = np.random.normal(0,.01,[output_size]) #hidden layer bias\n",
    "        self.vb = np.random.normal(0,.01,[input_size]) #input layer bias \n",
    "        \n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "        \n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    \n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
    "    \n",
    "    def train(self, X, teX):\n",
    "        _w = tf.placeholder(tf.float32, shape = [self.input_size, self.output_size])\n",
    "        _vb = tf.placeholder(tf.float32, shape = [self.input_size])\n",
    "        _hb = tf.placeholder(tf.float32, shape = [self.output_size])\n",
    "        print(f\"Shapes: _w = {_w.shape} _vb = {_vb.shape} _hb = {_hb.shape}\")\n",
    "\n",
    "        pre_w = np.random.normal(0,.01, size = [self.input_size,self.output_size])\n",
    "        pre_vb = np.random.normal(0, .01, size = [self.input_size])\n",
    "        pre_hb = np.random.normal(0, .01, size = [self.output_size])\n",
    "        \n",
    "        cur_w = np.random.normal(0, .01, size = [self.input_size,self.output_size])\n",
    "        cur_vb = np.random.normal(0, .01, size = [self.input_size])\n",
    "        cur_hb = np.random.normal(0, .01, size = [self.output_size])\n",
    "               \n",
    "        v0 = tf.placeholder(tf.float32, shape = [None, self.input_size])\n",
    "         \n",
    "        h0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb ))\n",
    "\n",
    "        v1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
    "\n",
    "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
    "        \n",
    "        positive_grad = tf.matmul(tf.transpose(v0), h0) #input' * hidden0\n",
    "        negative_grad = tf.matmul(tf.transpose(v1), h1) #reconstruction' * hidden1\n",
    "        \n",
    "        CD = (positive_grad - negative_grad) / tf.to_float(tf.shape(v0)[0]) \n",
    "\n",
    "        update_w = _w + self.learning_rate * CD\n",
    "        update_vb = _vb + tf.reduce_mean(v0 - v1, 0)\n",
    "        update_hb = _hb + tf.reduce_mean(h0 - h1, 0)\n",
    "        \n",
    "        #MSE\n",
    "        err = tf.reduce_mean(tf.square(v0 - v1))\n",
    "        \n",
    "        errors = []\n",
    "        hidden_units = []\n",
    "        reconstruction = []\n",
    "        \n",
    "        test_hidden_units = []\n",
    "        test_reconstruction=[]\n",
    "        \n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(self.epochs):\n",
    "                for start, end in zip(range(0, len(X), self.batch_size), range(self.batch_size, len(X), self.batch_size)):\n",
    "                    batch = X[start:end] #Mini batch of images taken from training data\n",
    "                    \n",
    "                    #Feed in batch, previous weights/bias, update weights and store them in current weights\n",
    "                    cur_w = sess.run(update_w, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
    "                    \n",
    "                    #Save weights \n",
    "                    pre_w = cur_w\n",
    "                    pre_hb = cur_hb\n",
    "                    pre_vb = cur_vb\n",
    "                \n",
    "                #At the end of each iteration, the reconstructed images are stored and the error is outputted \n",
    "                reconstruction.append(sess.run(v1, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb}))        \n",
    "                print('Learning Rate: {}:  Batch Size: {}:  Hidden Layers: {}: Epoch: {}: Error: {}:'.format(self.learning_rate, self.batch_size, \n",
    "                                                                                                             self.output_size, (epoch+1),\n",
    "                                                                                                            sess.run(err, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb})))\n",
    "            \n",
    "            #Store final reconstruction in RBM object\n",
    "            self.new_input_layer = reconstruction[-1]\n",
    "            \n",
    "            #Store weights in RBM object\n",
    "            self.w = pre_w\n",
    "            self.hb = pre_hb\n",
    "            self.vb = pre_vb\n",
    "    \n",
    "    def rbm_output(self, X):\n",
    "        input_x = tf.constant(X)\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        _vb = tf.constant(self.vb)\n",
    "        \n",
    "        out = tf.nn.sigmoid(tf.matmul(input_x, _w) + _hb)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            return sess.run(out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RBM_hidden_size = [600,500,100] #Three hidden layer sizes for our three layer DBN\n",
    "learning_rate = .01 \n",
    "input_size = trX.shape[1] #input layer size of original image data\n",
    "\n",
    "rbm_list = [] #This will hold all of the RBMs used in our DBN\n",
    "\n",
    "#Creates 3 RBMs\n",
    "for layer in RBM_hidden_size:\n",
    "    rbm_list.append(RBM(input_size, layer, learning_rate, 32))\n",
    "    input_size = layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  (60000, 784)\n",
      "Layer:  1\n",
      "Shapes: _w = (784, 600) _vb = (784,) _hb = (600,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01:  Batch Size: 32:  Hidden Layers: 600: Epoch: 1: Error: 7217.080078125:\n",
      "Learning Rate: 0.01:  Batch Size: 32:  Hidden Layers: 600: Epoch: 2: Error: 7217.080078125:\n",
      "Learning Rate: 0.01:  Batch Size: 32:  Hidden Layers: 600: Epoch: 3: Error: 7217.080078125:\n",
      "Learning Rate: 0.01:  Batch Size: 32:  Hidden Layers: 600: Epoch: 4: Error: 7217.080078125:\n",
      "Learning Rate: 0.01:  Batch Size: 32:  Hidden Layers: 600: Epoch: 5: Error: 7217.080078125:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input 'b' of 'MatMul' Op has type float32 that does not match type uint8 of argument 'a'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alaric\\Documents\\Code\\Thesis-Repository\\Run\\v1 Code.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alaric/Documents/Code/Thesis-Repository/Run/v1%20Code.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLayer: \u001b[39m\u001b[39m'\u001b[39m,(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alaric/Documents/Code/Thesis-Repository/Run/v1%20Code.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m rbm\u001b[39m.\u001b[39mtrain(inpX, teX)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Alaric/Documents/Code/Thesis-Repository/Run/v1%20Code.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m inpX \u001b[39m=\u001b[39m rbm\u001b[39m.\u001b[39;49mrbm_output(inpX)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alaric/Documents/Code/Thesis-Repository/Run/v1%20Code.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m test_inpx \u001b[39m=\u001b[39m rbm\u001b[39m.\u001b[39mrbm_output(test_inpx)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alaric/Documents/Code/Thesis-Repository/Run/v1%20Code.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m rbm_outputs\u001b[39m.\u001b[39mappend(inpX)\n",
      "\u001b[1;32mc:\\Users\\Alaric\\Documents\\Code\\Thesis-Repository\\Run\\v1 Code.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Alaric/Documents/Code/Thesis-Repository/Run/v1%20Code.ipynb#X13sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m _hb \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhb)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Alaric/Documents/Code/Thesis-Repository/Run/v1%20Code.ipynb#X13sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m _vb \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvb)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Alaric/Documents/Code/Thesis-Repository/Run/v1%20Code.ipynb#X13sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m out \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msigmoid(tf\u001b[39m.\u001b[39;49mmatmul(input_x, _w) \u001b[39m+\u001b[39m _hb)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Alaric/Documents/Code/Thesis-Repository/Run/v1%20Code.ipynb#X13sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m sess:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Alaric/Documents/Code/Thesis-Repository/Run/v1%20Code.ipynb#X13sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     sess\u001b[39m.\u001b[39mrun(tf\u001b[39m.\u001b[39mglobal_variables_initializer())\n",
      "File \u001b[1;32mc:\\Users\\Alaric\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Alaric\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:588\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[1;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m inferred_from:\n\u001b[0;32m    586\u001b[0m           inferred_from[k] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDefault in OpDef\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 588\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    589\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39m type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    590\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdtypes\u001b[39m.\u001b[39mas_dtype(attrs[input_arg\u001b[39m.\u001b[39mtype_attr])\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    591\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39margument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00minferred_from[input_arg\u001b[39m.\u001b[39mtype_attr]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m types \u001b[39m=\u001b[39m [values\u001b[39m.\u001b[39mdtype]\n\u001b[0;32m    594\u001b[0m inputs\u001b[39m.\u001b[39mappend(values)\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'b' of 'MatMul' Op has type float32 that does not match type uint8 of argument 'a'."
     ]
    }
   ],
   "source": [
    "#Initalize input layer variables \n",
    "inpX = trX                \n",
    "test_inpx = teX\n",
    "\n",
    "# DBN Loop\n",
    "for i,rbm in enumerate(rbm_list):\n",
    "    rbm_outputs = []\n",
    "    rbm_test_outputs = []\n",
    "    print('Input Shape: ', inpX.shape)\n",
    "    print('Layer: ',(i+1))\n",
    "\n",
    "    rbm.train(inpX, teX)\n",
    "    inpX = rbm.rbm_output(inpX)\n",
    "    test_inpx = rbm.rbm_output(test_inpx)\n",
    "    rbm_outputs.append(inpX)\n",
    "    rbm_test_outputs.append(test_inpx)\n",
    "\n",
    "    print('Output Shape: ', inpX.shape)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
